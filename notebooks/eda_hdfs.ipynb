{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS Log Analysis: Drift & Distribution\n",
    "\n",
    "Analyzes HDFS logs for:\n",
    "- Template frequency/entropy\n",
    "- Class imbalance\n",
    "- Spike detection (3σ)\n",
    "- PSI/KS week-over-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_hdfs_logs(log_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load HDFS logs into DataFrame.\"\"\"\n",
    "    records = []\n",
    "    for log_file in log_dir.glob('*.log'):\n",
    "        with open(log_file) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 4:\n",
    "                    ts_str, host, comp, msg, *rest = parts\n",
    "                    level = rest[0] if rest else None\n",
    "                    records.append({\n",
    "                        'ts': pd.to_datetime(ts_str),\n",
    "                        'host': host,\n",
    "                        'component': comp,\n",
    "                        'message': msg,\n",
    "                        'level': level\n",
    "                    })\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load logs\n",
    "log_dir = Path('../tests/data/hdfs')\n",
    "df = load_hdfs_logs(log_dir)\n",
    "print(f'Loaded {len(df):,} log entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Frequency Analysis\n",
    "\n",
    "Analyze template distribution and entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from drain3 import TemplateMiner\n",
    "\n",
    "# Extract templates\n",
    "miner = TemplateMiner()\n",
    "templates = {}\n",
    "for msg in df['message']:\n",
    "    result = miner.add_log_message(msg)\n",
    "    templates[msg] = result.cluster_id\n",
    "\n",
    "df['template_id'] = df['message'].map(templates)\n",
    "\n",
    "# Calculate frequencies\n",
    "template_counts = df['template_id'].value_counts()\n",
    "\n",
    "# Plot top 20 templates\n",
    "plt.figure(figsize=(15, 6))\n",
    "template_counts.head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Template Frequencies')\n",
    "plt.xlabel('Template ID')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Calculate entropy\n",
    "probs = template_counts / len(df)\n",
    "entropy = stats.entropy(probs)\n",
    "print(f'Template entropy: {entropy:.2f} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance Analysis\n",
    "\n",
    "Check distribution of log levels and components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Log level distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['level'].value_counts().plot(kind='bar')\n",
    "plt.title('Log Level Distribution')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Component distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "df['component'].value_counts().head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Components')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike Detection (3σ)\n",
    "\n",
    "Detect anomalous spikes in log volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Resample to 5-minute buckets\n",
    "ts_counts = df.set_index('ts').resample('5T').size()\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = ts_counts.mean()\n",
    "std = ts_counts.std()\n",
    "threshold = mean + 3*std\n",
    "\n",
    "# Find spikes\n",
    "spikes = ts_counts[ts_counts > threshold]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "ts_counts.plot()\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='3σ threshold')\n",
    "plt.scatter(spikes.index, spikes.values, color='red', label='Spikes')\n",
    "plt.title('Log Volume Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Logs per 5min')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f'Found {len(spikes)} spikes above 3σ threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSI/KS Week-over-Week Analysis\n",
    "\n",
    "Check for distribution drift between weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_psi(expected, actual):\n",
    "    \"\"\"Calculate Population Stability Index.\"\"\"\n",
    "    # Convert to probabilities\n",
    "    e_probs = expected / expected.sum()\n",
    "    a_probs = actual / actual.sum()\n",
    "    \n",
    "    # Handle zero probabilities\n",
    "    e_probs = e_probs.replace(0, 1e-6)\n",
    "    a_probs = a_probs.replace(0, 1e-6)\n",
    "    \n",
    "    # Calculate PSI\n",
    "    psi = ((a_probs - e_probs) * np.log(a_probs / e_probs)).sum()\n",
    "    return psi\n",
    "\n",
    "# Split into weeks\n",
    "df['week'] = df['ts'].dt.isocalendar().week\n",
    "\n",
    "# Get template distributions by week\n",
    "weekly_dists = {}\n",
    "for week in df['week'].unique():\n",
    "    weekly_dists[week] = df[df['week'] == week]['template_id'].value_counts()\n",
    "\n",
    "# Calculate PSI and KS test for each week pair\n",
    "weeks = sorted(weekly_dists.keys())\n",
    "for i in range(len(weeks)-1):\n",
    "    week1, week2 = weeks[i], weeks[i+1]\n",
    "    dist1, dist2 = weekly_dists[week1], weekly_dists[week2]\n",
    "    \n",
    "    # PSI\n",
    "    psi = calculate_psi(dist1, dist2)\n",
    "    \n",
    "    # KS test\n",
    "    ks_stat, p_val = stats.ks_2samp(\n",
    "        np.repeat(dist1.index, dist1.values),\n",
    "        np.repeat(dist2.index, dist2.values)\n",
    "    )\n",
    "    \n",
    "    print(f'Week {week1} vs {week2}:')\n",
    "    print(f'  PSI: {psi:.3f}')\n",
    "    print(f'  KS stat: {ks_stat:.3f} (p={p_val:.3e})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
